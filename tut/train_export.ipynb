{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from dotenv import load_dotenv\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nilmtk.losses\n",
    "import enilm.etypes\n",
    "import enilm.reports.sample_rate\n",
    "import enilm.appliances\n",
    "import enilm.datasets\n",
    "import enilm.seed\n",
    "import enilm.yaml.config\n",
    "import enilm.yaml.data\n",
    "import enilm.yaml.daily\n",
    "import enilm.yaml.daily.split\n",
    "import enilm.norm\n",
    "import enilm.windowing\n",
    "import enilm.models.torch.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Dataset Path\n",
    "\n",
    "Create `config.yaml` file:\n",
    "\n",
    "```yaml\n",
    "datasets:\n",
    "    paths:\n",
    "        {sys.platform}:\n",
    "            {socket.gethostname()}:\n",
    "                {getpass.getuser()}:\n",
    "                    REDD: /path/to/REDD.h5\n",
    "```\n",
    "\n",
    "Note that the h5 file must be loadable with `nilmtk`.\n",
    "\n",
    "Then set the `ENILM_CONFIG_FILE_PATH` environment variable to the path of the `config.yaml` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env EXP_NAME redd_fridge\n",
    "exp_name = os.environ[\"EXP_NAME\"]\n",
    "print(f\"exp_name: {exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "enilm.seed.set(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = enilm.etypes.Datasets.REDD.name\n",
    "house = 1\n",
    "\n",
    "sequence_length = 599\n",
    "batch_size = 512\n",
    "num_epochs = 25\n",
    "patience = 6\n",
    "\n",
    "save_models = True\n",
    "load_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilmtk_ds = nilmtk.DataSet(enilm.datasets.get_dataset_path(dataset))\n",
    "elec = nilmtk_ds.buildings[house].elec\n",
    "elec.appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = 'fridge'\n",
    "app_elec = enilm.appliances.get_elec(app_name, elec)\n",
    "app_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enilm.reports.sample_rate.sample_rate_info(elec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enilm.reports.sample_rate.sample_rate_info(app_elec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_params = enilm.yaml.config.ResampleParams(\n",
    "    rule=\"6s\", \n",
    "    fill=\"ffill\", \n",
    "    how=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_folder = Path(os.environ['CACHE_FOLDER'])\n",
    "print(cache_folder)\n",
    "assert cache_folder.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir_path = cache_folder / \"nbs\" / exp_name\n",
    "memory_cache_path = cache_dir_path / \"joblib\"\n",
    "models_cache_path = cache_dir_path / \"models\"\n",
    "exp_json_path = cache_dir_path / \"exp.json\"\n",
    "\n",
    "memory = joblib.Memory(memory_cache_path, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `/home/mbouchur/shared-projects/ma-embeddedml/enilm/enilm/nbs/preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = enilm.yaml.config.Config(\n",
    "    dataset=dataset,\n",
    "    house=house,\n",
    "    data_path=cache_dir_path,\n",
    "    selected_physical_quantity=\"power\",\n",
    "    selected_ac_type=enilm.yaml.config.ACTypes(mains='apparent', apps='active'),\n",
    "    selected_apps=[app_name],\n",
    "    selected_gpu=5,\n",
    "    resample_params=resample_params,\n",
    "    selected_train_percent=0.8,\n",
    "    manually_deleted_days=[],\n",
    "    selected_n_samples_per_day=None,\n",
    "    n_days=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for training and testing using the yaml-config API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@memory.cache()\n",
    "def get_data(config):\n",
    "    raw_data = enilm.yaml.data.raw(config)\n",
    "    resampled_data = enilm.yaml.data.resample(config)\n",
    "    overlapping_data = enilm.yaml.data.overlapping(config)\n",
    "    \n",
    "    each_day_cleaned = enilm.yaml.daily.clean(config)\n",
    "    each_day_cleaned_traintest = enilm.yaml.daily.split.train_test(each_day_cleaned, config)\n",
    "    xy_cleaned = enilm.yaml.daily.split.traintest_xy(each_day_cleaned_traintest)\n",
    "    \n",
    "    return raw_data, resampled_data, overlapping_data, each_day_cleaned, each_day_cleaned_traintest, xy_cleaned\n",
    "\n",
    "raw_data, resampled_data, overlapping_data, each_day_cleaned, each_day_cleaned_traintest, xy_cleaned = get_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_idx = 1\n",
    "\n",
    "days_dates = list(each_day_cleaned['mains'].keys())\n",
    "assert day_idx < len(days_dates)\n",
    "\n",
    "for key in each_day_cleaned.keys():\n",
    "    plt.plot(each_day_cleaned[key][days_dates[day_idx]], label=key)\n",
    "plt.title(days_dates[day_idx])\n",
    "plt.legend()\n",
    "plt.xticks(rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mains_norm_params = enilm.norm.compute(xy_cleaned.train_x)\n",
    "apps_norm_params = {\n",
    "    app: enilm.norm.compute(xy_cleaned.train_y[app])\n",
    "    for app in config.selected_apps\n",
    "}\n",
    "\n",
    "# x == mains, y == apps, train:test == split\n",
    "xy_cleaned_normalized = enilm.yaml.daily.split.XYNP(\n",
    "    train_x=enilm.norm.normalize(xy_cleaned.train_x, mains_norm_params),\n",
    "    test_x=enilm.norm.normalize(xy_cleaned.test_x, mains_norm_params),\n",
    "    train_y={app: enilm.norm.normalize(xy_cleaned.train_y[app], apps_norm_params[app]) for app in config.selected_apps},\n",
    "    test_y={app: enilm.norm.normalize(xy_cleaned.test_y[app], apps_norm_params[app]) for app in config.selected_apps},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2PDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mains: np.ndarray,\n",
    "        appliance: np.ndarray,\n",
    "        sequence_length: int,\n",
    "        pad=True,  # whether to pad the sequence or not (e.g. 1, 2, 3, 4, 5, ... -> 0, 0, 1, 2, 3 if pad is True for sequence_length=5 and 1, 2, 3, 4, 5 if pad is False)\n",
    "        reshape=True,  # whether to reshape the inputs and targets to the expected shape of the model\n",
    "    ):\n",
    "        assert sequence_length % 2 == 1\n",
    "        assert isinstance(mains, np.ndarray)\n",
    "        assert isinstance(appliance, np.ndarray)\n",
    "        assert mains.shape == appliance.shape\n",
    "        self.sequence_length = sequence_length\n",
    "        self.mains = torch.tensor(mains)\n",
    "        self.appliance = torch.tensor(appliance)\n",
    "        self.pad = pad\n",
    "        self.reshape = reshape\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.pad:\n",
    "            return len(self.mains)\n",
    "        return len(self.mains) - self.sequence_length + 1\n",
    "\n",
    "    def _reshape(self, mains, ground_truth):\n",
    "        return (\n",
    "            # reshape inputs: add a channel dimension\n",
    "            mains.reshape(-1, self.sequence_length),\n",
    "            # reshape targets: since the model outputs a single value per sequence, add the extra singularity dimension\n",
    "            ground_truth.reshape(\n",
    "                -1,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # stop iteration if idx is out of bounds\n",
    "        if idx == len(self.mains):\n",
    "            raise StopIteration\n",
    "\n",
    "        # padding or truncating\n",
    "        if self.pad:\n",
    "            mains, gt = (\n",
    "                utils.get_padded_sequence(self.mains, self.sequence_length, idx),\n",
    "                self.appliance[idx],\n",
    "            )\n",
    "        else:\n",
    "            if idx > len(self.mains) - self.sequence_length:\n",
    "                raise StopIteration\n",
    "            half_seq_len = self.sequence_length // 2\n",
    "            idx += half_seq_len\n",
    "            mains, gt = (\n",
    "                self.mains[idx - half_seq_len : idx + half_seq_len + 1],\n",
    "                self.appliance[idx],\n",
    "            )\n",
    "\n",
    "        # reshape?\n",
    "        if self.reshape:\n",
    "            mains, gt = self._reshape(mains, gt)\n",
    "\n",
    "        return mains, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2PDatasetMains(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mains: np.ndarray,\n",
    "        sequence_length: int,\n",
    "        pad=True,  # whether to pad the sequence or not (e.g. 1, 2, 3, 4, 5, ... -> 0, 0, 1, 2, 3 if pad is True for sequence_length=5 and 1, 2, 3, 4, 5 if pad is False)\n",
    "        reshape=True,  # whether to reshape the inputs and targets to the expected shape of the model\n",
    "    ):\n",
    "        assert sequence_length % 2 == 1\n",
    "        assert isinstance(mains, np.ndarray)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.mains = torch.tensor(mains)\n",
    "        self.pad = pad\n",
    "        self.reshape = reshape\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.pad:\n",
    "            return len(self.mains)\n",
    "        return len(self.mains) - self.sequence_length + 1\n",
    "\n",
    "    def _reshape(self, mains):\n",
    "        # reshape inputs: add a channel dimension\n",
    "        return mains.reshape(-1, self.sequence_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # stop iteration if idx is out of bounds\n",
    "        if idx == len(self.mains):\n",
    "            raise StopIteration\n",
    "\n",
    "        # padding or truncating\n",
    "        if self.pad:\n",
    "            mains = enilm.models.torch.utils.get_padded_sequence(\n",
    "                self.mains, self.sequence_length, idx\n",
    "            )\n",
    "        else:\n",
    "            if idx > len(self.mains) - self.sequence_length:\n",
    "                raise StopIteration\n",
    "            half_seq_len = self.sequence_length // 2\n",
    "            idx += half_seq_len\n",
    "            mains = self.mains[idx - half_seq_len : idx + half_seq_len + 1]\n",
    "\n",
    "        # reshape?\n",
    "        if self.reshape:\n",
    "            mains = self._reshape(mains)\n",
    "\n",
    "        return mains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert app_name in each_day_cleaned.keys()\n",
    "\n",
    "train_dataset = S2PDataset(\n",
    "    mains=xy_cleaned_normalized.train_x,\n",
    "    appliance=xy_cleaned_normalized.train_y[app_name],\n",
    "    sequence_length=sequence_length,\n",
    "    pad=True,\n",
    "    reshape=True,\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = S2PDataset(\n",
    "    mains=xy_cleaned_normalized.test_x,\n",
    "    appliance=xy_cleaned_normalized.test_y[app_name],\n",
    "    sequence_length=sequence_length,\n",
    "    pad=True,\n",
    "    reshape=True,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S2P(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(S2P, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,  out_channels=30, kernel_size=10, stride=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=30, out_channels=30, kernel_size=8,  stride=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=30, out_channels=40, kernel_size=6,  stride=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=40, out_channels=50, kernel_size=5,  stride=1)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.conv5 = nn.Conv1d(in_channels=50, out_channels=50, kernel_size=5,  stride=1)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=((seq_len - self.conv1.out_channels) + 1) * self.conv5.out_channels, \n",
    "            out_features=1024,\n",
    "        )\n",
    "        # self.fc1 = nn.Linear(50 * ((seq_len - 4*10 - 3*8 - 2*6 - 5*1 + 12) // 1), 1024)\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(in_features=self.fc1.out_features, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = S2P(sequence_length)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f'cuda:{config.selected_gpu}')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Moving model to device\n",
    "model = S2P(sequence_length).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses before training (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "input_data = np.array(\n",
    "    enilm.norm.normalize(\n",
    "        overlapping_data[\"mains\"],\n",
    "        mains_norm_params,\n",
    "    )\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    S2PDatasetMains(\n",
    "        mains=input_data,\n",
    "        sequence_length=sequence_length,\n",
    "        pad=True,\n",
    "        reshape=True,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "preds = []\n",
    "for inputs in data_loader:\n",
    "    inputs = inputs.to(torch.float32).to(device)\n",
    "    preds.append(model(inputs).cpu().detach().numpy().flatten())\n",
    "preds = np.concatenate(preds)\n",
    "pr_np: np.ndarray = enilm.norm.denormalize(preds, apps_norm_params[app_name])\n",
    "\n",
    "gt_np = overlapping_data[app_name].to_numpy()\n",
    "train_size = int(config.selected_train_percent * gt_np.size)\n",
    "\n",
    "# MAE\n",
    "loss_no_training = {}\n",
    "loss_no_training[\"mae\"] = {}\n",
    "loss_no_training[\"mae\"][\"train\"] = float(np.mean(np.abs(pr_np[:train_size] - gt_np[:train_size])))\n",
    "loss_no_training[\"mae\"][\"test\"] = float(np.mean(np.abs(pr_np[train_size:] - gt_np[train_size:])))\n",
    "print(f\"Train MAE: {loss_no_training['mae']['train']:.2f} Watts\")\n",
    "print(f\"Test MAE: {loss_no_training['mae']['test']:.2f} Watts\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model_ep = None\n",
    "patience_ctr = patience\n",
    "bias_dtype = model.conv1.bias.dtype\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    model_ep_path = models_cache_path / f\"model_ep{epoch + 1}.pt\"\n",
    "    ep_train_loss_path = model_ep_path.parent / f\"train_loss_ep{epoch + 1}.txt\"\n",
    "    ep_val_loss_path = model_ep_path.parent / f\"val_loss_ep{epoch + 1}.txt\"\n",
    "\n",
    "    # Create the directory if it does not exist\n",
    "    if save_models and not models_cache_path.exists():\n",
    "        models_cache_path.mkdir(parents=True)\n",
    "\n",
    "    # Load from saved model if available\n",
    "    if model_ep_path.exists() and load_models:\n",
    "        print(f\"Loading model from epoch {epoch + 1}\")\n",
    "        model.load_state_dict(torch.load(model_ep_path))\n",
    "        train_loss = float(ep_train_loss_path.read_text())\n",
    "    # Train for one epoch\n",
    "    else:\n",
    "        # Set the model to train mode i.e. gradient tracking is on\n",
    "        model.train(True)\n",
    "\n",
    "        train_loss = []\n",
    "        for step, (inputs, targets) in enumerate(train_loader):\n",
    "            curr_batch_size = inputs.size(0)\n",
    "\n",
    "            # move data to GPU\n",
    "            inputs, targets = inputs.to(bias_dtype).to(device), targets.to(bias_dtype).to(device)\n",
    "\n",
    "            # Zero gradients for every batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights (parameter updates)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())  # * curr_batch_size\n",
    "            # By multiplying loss.item() with inputs.size(0), you obtain the total loss for the current batch.\n",
    "            # The reason for multiplying by inputs.size(0) is to account for variations in batch sizes. If you have\n",
    "            # a batch size of 32, for example, multiplying the loss by 32 will give you the total loss for that batch.\n",
    "            # By summing the loss for each batch and keeping track of the total number of samples processed\n",
    "            # (len(train_loader.dataset)), you can compute the average training loss for the entire dataset by dividing\n",
    "            # train_loss by the total number of samples\n",
    "\n",
    "        # Compute the average training loss for the epoch\n",
    "        train_loss = np.mean(train_loss)\n",
    "\n",
    "        if save_models:\n",
    "            # Check if the model already exists\n",
    "            if model_ep_path.exists():\n",
    "                print(f\"Overwriting model for epoch {epoch + 1}\")\n",
    "\n",
    "            # Save the loss for this epoch\n",
    "            ep_train_loss_path.write_text(str(train_loss))\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), model_ep_path)\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Set the model to evaluation mode i.e. disabling dropout and using population statistics for batch normalization\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    if load_models and ep_val_loss_path.exists():\n",
    "        val_loss = float(ep_val_loss_path.read_text())\n",
    "    else:\n",
    "        with torch.no_grad():  # Disable gradient computation and reduce memory consumption\n",
    "            for inputs, targets in val_loader:\n",
    "                curr_batch_size = inputs.size(0)\n",
    "\n",
    "                # move data to GPU\n",
    "                inputs, targets = inputs.to(bias_dtype).to(device), targets.to(bias_dtype).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = loss_fn(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()  # * curr_batch_size\n",
    "\n",
    "        # Compute the average validation loss for the epoch\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if save_models:\n",
    "            # Save the loss for this epoch\n",
    "            ep_val_loss_path.write_text(str(val_loss))\n",
    "\n",
    "    # Print the loss for each epoch\n",
    "    if train_loss is not None:\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Append to the loss history\n",
    "    loss_history.append(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"val\": val_loss,\n",
    "            \"train\": train_loss,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_ep = epoch\n",
    "        patience_ctr = patience  # reset patience counter\n",
    "    else:\n",
    "        patience_ctr -= 1\n",
    "        if patience_ctr == 0:\n",
    "            if best_model_ep is None:\n",
    "                print(\n",
    "                    \"Early stopping before any model was saved! \"\n",
    "                    \"-> continuing to next epoch even if val loss is increasing \"\n",
    "                    \"in the hope that it will decrease again\"\n",
    "                )\n",
    "                continue\n",
    "            else:\n",
    "                best_mode_path = models_cache_path / f\"model_ep{best_model_ep + 1}.pt\"\n",
    "                model.load_state_dict(torch.load(best_mode_path))\n",
    "                print(\"Early stopping at epoch\", epoch + 1)\n",
    "                print(\"Loading best model from epoch\", best_model_ep + 1)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = pd.DataFrame(loss_history)\n",
    "sns.lineplot(data=loss_history, x='epoch', y='train', label='Train loss')\n",
    "sns.lineplot(data=loss_history, x='epoch', y='val', label='Validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses after training (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "input_data = np.array(\n",
    "    enilm.norm.normalize(\n",
    "        overlapping_data[\"mains\"],\n",
    "        mains_norm_params,\n",
    "    )\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    S2PDatasetMains(\n",
    "        mains=input_data,\n",
    "        sequence_length=sequence_length,\n",
    "        pad=True,\n",
    "        reshape=True,\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "preds = []\n",
    "for inputs in data_loader:\n",
    "    inputs = inputs.to(torch.float32).to(device)\n",
    "    preds.append(model(inputs).cpu().detach().numpy().flatten())\n",
    "preds = np.concatenate(preds)\n",
    "pr_np: np.ndarray = enilm.norm.denormalize(preds, apps_norm_params[app_name])\n",
    "\n",
    "gt_np = overlapping_data[app_name].to_numpy()\n",
    "train_size = int(config.selected_train_percent * gt_np.size)\n",
    "\n",
    "# MAE\n",
    "loss = {}\n",
    "loss[\"mae\"] = {}\n",
    "loss[\"mae\"][\"train\"] = float(np.mean(np.abs(pr_np[:train_size] - gt_np[:train_size])))\n",
    "loss[\"mae\"][\"test\"] = float(np.mean(np.abs(pr_np[train_size:] - gt_np[train_size:])))\n",
    "print(\n",
    "    f\"Train MAE: {loss['mae']['train']:.2f} Watts (before: {loss_no_training['mae']['train']:.2f} Watts)\"\n",
    ")\n",
    "print(\n",
    "    f\"Test MAE: {loss['mae']['test']:.2f} Watts (before: {loss_no_training['mae']['test']:.2f} Watts)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for a single day\n",
    "day_idx = 3\n",
    "test_days_keys = list(each_day_cleaned_traintest.test['mains'])\n",
    "assert day_idx < len(test_days_keys)\n",
    "day_date = test_days_keys[day_idx]\n",
    "\n",
    "day_loader = DataLoader(S2PDataset(\n",
    "    mains=enilm.norm.normalize(each_day_cleaned_traintest.test['mains'][day_date].to_numpy(), mains_norm_params),\n",
    "    appliance=enilm.norm.normalize(each_day_cleaned_traintest.test[app_name][day_date].to_numpy(), apps_norm_params[app_name]),\n",
    "    sequence_length=sequence_length,\n",
    "    reshape=True,\n",
    "    pad=True,\n",
    "), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "preds = []\n",
    "best_preds = []\n",
    "for inputs, _ in day_loader:\n",
    "    preds.append(model(inputs.to(torch.float32)))\n",
    "\n",
    "preds_np = [p.data.numpy() for p in preds]\n",
    "preds_flat = np.concatenate(preds_np).flatten()\n",
    "preds_flat_denorm = enilm.norm.denormalize(preds_flat, apps_norm_params[app_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# plot mains\n",
    "fig.add_scatter(\n",
    "    x=each_day_cleaned_traintest.test[\"mains\"][day_date].index,\n",
    "    y=each_day_cleaned_traintest.test[\"mains\"][day_date].values,\n",
    "    name='mains',\n",
    ")\n",
    "\n",
    "# plot ground truth\n",
    "fig.add_scatter(\n",
    "    x=each_day_cleaned_traintest.test[app_name][day_date].index,\n",
    "    y=each_day_cleaned_traintest.test[app_name][day_date].values,\n",
    "    name='ground truth',\n",
    ")\n",
    "\n",
    "# plot predictions\n",
    "fig.add_scatter(\n",
    "    x=each_day_cleaned_traintest.test[app_name][day_date].index,\n",
    "    y=preds_flat_denorm,\n",
    "    name='predictions last',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Exp to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/backend/types/exp.py\n",
    "\n",
    "class Exp(BaseModel):\n",
    "    model_config = ConfigDict(protected_namespaces=())\n",
    "    dataset: str  # enilm.etypes.DatasetID\n",
    "    house: enilm.etypes.HouseNr\n",
    "    app: enilm.etypes.AppName\n",
    "    exp_name: str\n",
    "    app_norm_params: enilm.norm.NormParams\n",
    "    mains_norm_params: enilm.norm.NormParams\n",
    "    selected_ac_type: enilm.yaml.config.ACTypes\n",
    "    resample_params: enilm.yaml.config.ResampleParams\n",
    "    on_power_threshold: float = 6.0  # watts\n",
    "    description: str = \"\"\n",
    "\n",
    "\n",
    "class ModelExp(Exp):  # Inherit from DataExp\n",
    "    selected_model_weights: str\n",
    "    sequence_length: int\n",
    "    selected_train_percent: float\n",
    "    batch_size: int\n",
    "    num_epochs: int\n",
    "    model_name: str\n",
    "    model_class: str = \"enilm.models.torch.seq.S2P\"\n",
    "    ds_class: str = \"enilm.models.torch.seq.S2PDatasetMains\"\n",
    "\n",
    "if 'best_mode_path' not in globals():\n",
    "    print(\"Using last model as the best model since early stopping did not trigger\")\n",
    "    best_mode_path = Path(models_cache_path / f\"model_ep{num_epochs}.pt\")\n",
    "\n",
    "exp = ModelExp(\n",
    "    dataset=dataset,\n",
    "    house=house,\n",
    "    app=app_name,\n",
    "    exp_name=exp_name,\n",
    "    app_norm_params=apps_norm_params[app_name],\n",
    "    mains_norm_params=mains_norm_params,\n",
    "    selected_model_weights=best_mode_path.name,\n",
    "    sequence_length=sequence_length,\n",
    "    selected_ac_type=config.selected_ac_type,\n",
    "    resample_params=config.resample_params,\n",
    "    selected_train_percent=config.selected_train_percent,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=num_epochs,\n",
    "    model_name='S2P',\n",
    "    description='',\n",
    ")\n",
    "exp_json_path.write_text(exp.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
